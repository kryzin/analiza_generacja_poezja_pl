{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Text Generation NLP – Everything You Need To Know / Python Code To Get Started](https://spotintelligence.com/2022/12/19/text-generation-nlp/)\n",
    "Here is an example of how you could use the **NLTK library** to train a simple generative model for text using a **bigram language model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\karol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that']\n",
      "True\n",
      "recent years ago , and the same time , and the\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "\n",
    "nltk.download('brown')\n",
    "# Load and preprocess the data\n",
    "text = brown.words()\n",
    "\n",
    "# Create a bigram language model\n",
    "bigrams = nltk.bigrams(text)\n",
    "cfd = ConditionalFreqDist(bigrams)\n",
    "print(list(cfd.keys())[:20])  # Print the first 20 keys in cfd\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"recent\"\n",
    "generated_text = seed_text\n",
    "print(seed_text in cfd)\n",
    "for i in range(10):\n",
    "    # Find the next word using the bigram model\n",
    "    next_word = cfd[seed_text].max()\n",
    "    generated_text += \" \" + next_word\n",
    "    seed_text = next_word\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.0704 - loss: 7.5890\n",
      "Epoch 2/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.0693 - loss: 6.9798\n",
      "Epoch 3/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.0776 - loss: 6.8593\n",
      "Epoch 4/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.0844 - loss: 6.7278\n",
      "Epoch 5/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.0910 - loss: 6.5291\n",
      "Epoch 6/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.0916 - loss: 6.3655\n",
      "Epoch 7/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.0989 - loss: 6.1863\n",
      "Epoch 8/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.1096 - loss: 5.9873\n",
      "Epoch 9/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.1171 - loss: 5.8015\n",
      "Epoch 10/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.1236 - loss: 5.6444\n",
      "Epoch 11/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.1280 - loss: 5.5047\n",
      "Epoch 12/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.1365 - loss: 5.3303\n",
      "Epoch 13/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.1420 - loss: 5.1905\n",
      "Epoch 14/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.1494 - loss: 5.0723\n",
      "Epoch 15/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.1572 - loss: 4.9373\n",
      "Epoch 16/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.1643 - loss: 4.8193\n",
      "Epoch 17/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.1736 - loss: 4.7117\n",
      "Epoch 18/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.1876 - loss: 4.5878\n",
      "Epoch 19/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.1942 - loss: 4.4950\n",
      "Epoch 20/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.2104 - loss: 4.3854\n",
      "Epoch 21/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.2239 - loss: 4.2619\n",
      "Epoch 22/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.2441 - loss: 4.1457\n",
      "Epoch 23/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.2505 - loss: 4.0788\n",
      "Epoch 24/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.2637 - loss: 3.9920\n",
      "Epoch 25/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.2751 - loss: 3.9053\n",
      "Epoch 26/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.2892 - loss: 3.8174\n",
      "Epoch 27/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.2964 - loss: 3.7515\n",
      "Epoch 28/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.3076 - loss: 3.6683\n",
      "Epoch 29/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.3183 - loss: 3.6094\n",
      "Epoch 30/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.3305 - loss: 3.5240\n",
      "Epoch 31/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.3354 - loss: 3.4585\n",
      "Epoch 32/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.3470 - loss: 3.3924\n",
      "Epoch 33/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.3595 - loss: 3.3286\n",
      "Epoch 34/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.3672 - loss: 3.2700\n",
      "Epoch 35/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.3731 - loss: 3.2157\n",
      "Epoch 36/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.3821 - loss: 3.1675\n",
      "Epoch 37/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.3886 - loss: 3.1224\n",
      "Epoch 38/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.3976 - loss: 3.0621\n",
      "Epoch 39/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.4057 - loss: 3.0119\n",
      "Epoch 40/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.4114 - loss: 2.9781\n",
      "Epoch 41/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.4234 - loss: 2.9186\n",
      "Epoch 42/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.4287 - loss: 2.8770\n",
      "Epoch 43/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.4336 - loss: 2.8460\n",
      "Epoch 44/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.4380 - loss: 2.8095\n",
      "Epoch 45/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.4506 - loss: 2.7587\n",
      "Epoch 46/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.4480 - loss: 2.7346\n",
      "Epoch 47/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.4606 - loss: 2.6867\n",
      "Epoch 48/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.4668 - loss: 2.6453\n",
      "Epoch 49/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.4713 - loss: 2.6239\n",
      "Epoch 50/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.4816 - loss: 2.5700\n",
      "Epoch 51/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.4844 - loss: 2.5515\n",
      "Epoch 52/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.4865 - loss: 2.5176\n",
      "Epoch 53/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.4896 - loss: 2.4940\n",
      "Epoch 54/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.4956 - loss: 2.4718\n",
      "Epoch 55/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5057 - loss: 2.4340\n",
      "Epoch 56/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.5059 - loss: 2.4198\n",
      "Epoch 57/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5143 - loss: 2.3797\n",
      "Epoch 58/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5121 - loss: 2.3663\n",
      "Epoch 59/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.5215 - loss: 2.3339\n",
      "Epoch 60/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5209 - loss: 2.3187\n",
      "Epoch 61/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5294 - loss: 2.2814\n",
      "Epoch 62/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5383 - loss: 2.2535\n",
      "Epoch 63/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5374 - loss: 2.2361\n",
      "Epoch 64/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.5382 - loss: 2.2183\n",
      "Epoch 65/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5463 - loss: 2.1954\n",
      "Epoch 66/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5473 - loss: 2.1772\n",
      "Epoch 67/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5466 - loss: 2.1768\n",
      "Epoch 68/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5540 - loss: 2.1405\n",
      "Epoch 69/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.5558 - loss: 2.1224\n",
      "Epoch 70/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.5594 - loss: 2.1102\n",
      "Epoch 71/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.5683 - loss: 2.0649\n",
      "Epoch 72/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5652 - loss: 2.0754\n",
      "Epoch 73/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5688 - loss: 2.0666\n",
      "Epoch 74/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5777 - loss: 2.0215\n",
      "Epoch 75/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5735 - loss: 2.0203\n",
      "Epoch 76/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.5797 - loss: 2.0046\n",
      "Epoch 77/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5795 - loss: 1.9936\n",
      "Epoch 78/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5866 - loss: 1.9663\n",
      "Epoch 79/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5883 - loss: 1.9534\n",
      "Epoch 80/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5862 - loss: 1.9442\n",
      "Epoch 81/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5883 - loss: 1.9447\n",
      "Epoch 82/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5940 - loss: 1.8967\n",
      "Epoch 83/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.5999 - loss: 1.8870\n",
      "Epoch 84/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5992 - loss: 1.8940\n",
      "Epoch 85/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6038 - loss: 1.8677\n",
      "Epoch 86/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6027 - loss: 1.8606\n",
      "Epoch 87/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6073 - loss: 1.8431\n",
      "Epoch 88/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6080 - loss: 1.8421\n",
      "Epoch 89/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.6094 - loss: 1.8221\n",
      "Epoch 90/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6136 - loss: 1.8063\n",
      "Epoch 91/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.6131 - loss: 1.8037\n",
      "Epoch 92/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6196 - loss: 1.7788\n",
      "Epoch 93/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.6129 - loss: 1.7899\n",
      "Epoch 94/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.6226 - loss: 1.7625\n",
      "Epoch 95/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.6276 - loss: 1.7431\n",
      "Epoch 96/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6287 - loss: 1.7383\n",
      "Epoch 97/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6201 - loss: 1.7588\n",
      "Epoch 98/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6249 - loss: 1.7314\n",
      "Epoch 99/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.6343 - loss: 1.7101\n",
      "Epoch 100/100\n",
      "\u001b[1m1415/1415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6324 - loss: 1.7133\n",
      "This is an crews for taking 45 means to distribute its action or\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Load and preprocess the data\n",
    "# text = \"This is an example of some text that we want to use to train a generative model.\"\n",
    "text = \" \".join(brown.words()[:50000])\n",
    "\n",
    "# Tokenize the text and create a vocabulary\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert the text to a sequence of word indices\n",
    "sequences = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "# Create input-output pairs\n",
    "sequence_length = 10\n",
    "input_sequences = []\n",
    "for i in range(len(sequences) - sequence_length):\n",
    "    input_sequences.append(sequences[i:i + sequence_length + 1])\n",
    "\n",
    "# Convert to NumPy array\n",
    "input_sequences = np.array(input_sequences)\n",
    "\n",
    "# Split into X (inputs) and y (outputs)\n",
    "X = input_sequences[:, :-1]  # Inputs: all except the last word\n",
    "y = input_sequences[:, -1]   # Outputs: the last word\n",
    "\n",
    "# One-hot encode the outputs\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=10, input_length=sequence_length))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=100, verbose=1)\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"This is an\"\n",
    "for i in range(10):\n",
    "    # Encode the seed text as a sequence of word indices\n",
    "    seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    seed_sequence = pad_sequences([seed_sequence], maxlen=sequence_length, padding='pre')\n",
    "    # Predict the next word\n",
    "    next_word_probs = model.predict(seed_sequence, verbose=0)[0]\n",
    "    next_word_idx = np.argmax(next_word_probs)\n",
    "    next_word = tokenizer.index_word.get(next_word_idx, '')  # Safely fetch word\n",
    "    seed_text += \" \" + next_word\n",
    "\n",
    "print(seed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is caught the vital departments of the legislature mississippi's mitchell the\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"It is\"\n",
    "for i in range(10):\n",
    "    # Encode the seed text as a sequence of word indices\n",
    "    seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    seed_sequence = pad_sequences([seed_sequence], maxlen=sequence_length, padding='pre')\n",
    "    # Predict the next word\n",
    "    next_word_probs = model.predict(seed_sequence, verbose=0)[0]\n",
    "    next_word_idx = np.argmax(next_word_probs)\n",
    "    next_word = tokenizer.index_word.get(next_word_idx, '')  # Safely fetch word\n",
    "    seed_text += \" \" + next_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "I need **a lot** of data, and storage for this data, consider other file formats, for big data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
